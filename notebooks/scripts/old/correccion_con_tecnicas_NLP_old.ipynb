{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos el script para obtener las transcripciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar transcripcines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripciones extraidas correctamente y guardadas en la carpeta 'transcripciones' (en la carpeta de notebooks).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ejecutamos el script crear_transcripciones.py\n",
    "%run /home/jovyan/work/scripts/crear_transcripciones.py\n",
    "\n",
    "# Para listar archivos en el directorio actual\n",
    "# print(\"Archivos en el directorio actual:\")\n",
    "# print(os.listdir('/home/jovyan/work'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from num2words import num2words\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar etiquetas HTML\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Eliminar los sonidos externos entre corchetes\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Eliminar los sonidos externos marcados con *\n",
    "    text = re.sub(r'\\*.*?\\*', '', text)\n",
    "    # Eliminar los sonidos externos entre paréntesis\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    # Convertir números a texto en español\n",
    "    text = re.sub(r'\\d+', lambda x: num2words(int(x.group()), lang='es'), text)\n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'[^A-Za-záéíóúüñÁÉÍÓÚÜÑ\\s]', '', text)\n",
    "    # Eliminar múltiples espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/transcripciones'\n",
    "output_directory = '/home/jovyan/work/data/cleaned'\n",
    "\n",
    "# Guardamos los archivos\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = clean_text(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres especiales pero conservar letras acentuadas, espacios y signos de puntuación básicos\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s.,]', '', text)\n",
    "    return text\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/cleaned'\n",
    "output_directory = '/home/jovyan/work/data/normalized'\n",
    "\n",
    "# Guardamos los archivos\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            normalized_text = normalize(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección ortográfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# spell = SpellChecker(language='es')\n",
    "\n",
    "# def correct_spelling(text):\n",
    "#     corrected_text = []\n",
    "#     for word in text.split():\n",
    "#         corrected_word = spell.correction(word)\n",
    "#         if corrected_word is not None:\n",
    "#             corrected_text.append(corrected_word)\n",
    "#     return ' '.join(corrected_text)\n",
    "\n",
    "# input_directory = '/home/jovyan/work/data/normalized'\n",
    "# output_directory = '/home/jovyan/work/data/corrected'\n",
    "\n",
    "# if not os.path.exists(output_directory):\n",
    "#     os.makedirs(output_directory)\n",
    "\n",
    "# for filename in os.listdir(input_directory):\n",
    "#     if filename.endswith('.txt'):\n",
    "#         with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "#             text = file.read()\n",
    "#             spelled_text = correct_spelling(text)\n",
    "#             with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "#                 out_file.write(spelled_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# input_directory = '/home/jovyan/work/data/corrected'\n",
    "input_directory = '/home/jovyan/work/data/normalized'\n",
    "output_directory = '/home/jovyan/work/data/tokenized'\n",
    "\n",
    "# Guardamos los archivos tokenizados\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/tokenized'\n",
    "output_directory = '/home/jovyan/work/data/no_stopwords'\n",
    "\n",
    "# Guardamos los archivos sin las stopwords\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = remove_stopwords(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def lemmatize_with_context(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PRON\", \"ADJ\", \"DET\"]:\n",
    "            lemmatized_tokens.append(token.text)\n",
    "        else:\n",
    "            lemmatized_tokens.append(token.lemma_)\n",
    "    \n",
    "    lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# input_directory = '/home/jovyan/work/data/no_stopwords'\n",
    "input_directory = '/home/jovyan/work/data/tokenized'\n",
    "output_directory = '/home/jovyan/work/data/lemmatized'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            lemmatized_text = lemmatize_with_context(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstrucción Gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def reconstruct_grammar(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        words = []\n",
    "        for token in sent:\n",
    "            words.append((token.i, token.text, token.dep_, token.head.i, token.pos_))\n",
    "        \n",
    "        # Mantener el orden de las palabras según las dependencias gramaticales\n",
    "        words_sorted = sorted(words, key=lambda x: x[0])\n",
    "        \n",
    "        # Reconstruir la oración manteniendo la coherencia gramatical\n",
    "        reordered_sentence = \" \".join([word[1] for word in words_sorted])\n",
    "        sentences.append(reordered_sentence)\n",
    "\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/tokenized'\n",
    "output_directory = '/home/jovyan/work/data/reconstructed'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            reconstructed_text = reconstruct_grammar(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(reconstructed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizamos las frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos reorganizados y guardados en el directorio 'reestructured'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función para reorganizar las palabras en frases coherentes\n",
    "def reorganize_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        words = []\n",
    "        for token in sent:\n",
    "            if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"DET\", \"ADP\", \"CONJ\", \"CCONJ\", \"SCONJ\"]:\n",
    "                words.append((token.i, token.text, token.dep_))\n",
    "        \n",
    "        # Mantener el orden de las palabras según las dependencias gramaticales\n",
    "        words_sorted = sorted(words, key=lambda x: x[0])\n",
    "        reordered_sentence = \" \".join([word[1] for word in words_sorted])\n",
    "        sentences.append(reordered_sentence)\n",
    "\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/reconstructed'\n",
    "output_directory = '/home/jovyan/work/data/reestructured'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Procesar los archivos en el directorio de entrada\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            reorganized_text = reorganize_sentences(text)\n",
    "            \n",
    "            # Guardar el texto reorganizado en el directorio de salida\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(reorganized_text)\n",
    "\n",
    "print(\"Textos reorganizados y guardados en el directorio 'reestructured'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Resultados\n",
    "### Comparación de Textos Originales y Corregidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: mi gente de Perú qué tal estáis ayer nos quedamos bueno el otro día nos quedamos eh hostias es el ca\n",
      "Corrected: mi gente de qué tal ayer nos quedamos bueno el otro día nos quedamos hostias el caballo vale no tiem\n",
      "==================================================\n",
      "Original: [Música] [Música] [Música] más [Música] voy a explicarte todo lo que te extraño te extraño demasiado\n",
      "Corrected: más a explicarte todo lo que te extraño te extraño demasiado ahora como tan diario yo yo ya juegan a\n",
      "==================================================\n",
      "Original: Estamos en uno de los trenes\n",
      "más costosos de todo el mundo. Tiene spa, restaurantes, bar, terraza, u\n",
      "Corrected: en uno de los trenes más costosos de todo el mundo tiene spa bar terraza una suite digna de un hotel\n",
      "==================================================\n",
      "Original: Qué tal eh sazonado con tinta con el sabor de las noticias del día de hoy a la madre güey hay carril\n",
      "Corrected: qué tal sazonado con tinta con el sabor de las noticias del día de hoy a la madre güey carriles tú a\n",
      "==================================================\n",
      "Original: Qué pasa chicos eh estoy aquí un poco a oscuras y escondido porque bueno para poneros en contexto el\n",
      "Corrected: qué pasa chicos aquí un poco a oscuras y escondido porque bueno para poneros en contexto el otro día\n",
      "==================================================\n",
      "Original: [Música] hola hoy tú que te crees que no te extraño nada extraño muchísimo en verdad pero aquí estam\n",
      "Corrected: hola hoy tú que te crees que no te extraño nada extraño muchísimo en verdad pero aquí estamos para u\n",
      "==================================================\n",
      "Original: [Música] esto es importante para cada adulto o cada persona que de temprano se sienta adulta porque \n",
      "Corrected: esto importante para cada adulto o cada persona que de temprano se sienta adulta porque marca el fin\n",
      "==================================================\n",
      "Original: ya hice un top al respecto hace muchos años y leer los comentarios de personas con el don de la imag\n",
      "Corrected: ya hice un top al respecto hace muchos años y leer los comentarios de personas con el don de la imag\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Función para leer los primeros n caracteres de un archivo\n",
    "def read_first_n_chars(file_path, n):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        return text[:n]\n",
    "\n",
    "# Función para mostrar la comparación entre el texto original y el corregido\n",
    "def compare_texts(original, corrected):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Función principal para comparar textos en archivos\n",
    "def compare_files(original_directory, corrected_directory, num_chars):\n",
    "    # Obtener los nombres de los archivos en el directorio original\n",
    "    file_names = [f for f in os.listdir(original_directory) if f.endswith('.txt')]\n",
    "\n",
    "    # Aplicar la comparación a los archivos de texto\n",
    "    for file_name in file_names:\n",
    "        original_path = os.path.join(original_directory, file_name)\n",
    "        corrected_path = os.path.join(corrected_directory, file_name)\n",
    "        \n",
    "        # Verificar que el archivo corregido exista\n",
    "        if os.path.exists(corrected_path):\n",
    "            original_text = read_first_n_chars(original_path, num_chars)\n",
    "            corrected_text = read_first_n_chars(corrected_path, num_chars)\n",
    "            \n",
    "            # Comparar los textos\n",
    "            compare_texts(original_text, corrected_text)\n",
    "        else:\n",
    "            print(f\"Archivo corregido no encontrado para: {file_name}\")\n",
    "\n",
    "# Directorios de transcripciones originales y corregidas\n",
    "original_directory = '/home/jovyan/work/data/transcripciones'\n",
    "corrected_directory = '/home/jovyan/work/data/reestructured'\n",
    "\n",
    "# Número de caracteres a comparar (puedes cambiar este valor según necesites)\n",
    "num_chars_to_compare = 100\n",
    "\n",
    "# Llamada a la función principal\n",
    "compare_files(original_directory, corrected_directory, num_chars_to_compare)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
