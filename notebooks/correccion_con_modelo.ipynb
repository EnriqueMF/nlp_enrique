{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar Recursos del Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 06:33:18.495423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:18.495443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 06:33:20.027543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-07-13 06:33:20.027861: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:20.027927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:20.027989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:20.028051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:20.062360: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-07-13 06:33:20.062564: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPUs disponibles:\", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA está disponible. Número de GPUs: 1\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA está disponible. Número de GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA no está disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de CPUs (os.cpu_count()): 12\n",
      "Número de CPUs (multiprocessing.cpu_count()): 12\n",
      "CUDA está disponible. Número de GPUs: 1\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "CUDA no está disponible en TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "def check_cpus():\n",
    "    print(\"Número de CPUs (os.cpu_count()):\", os.cpu_count())\n",
    "    print(\"Número de CPUs (multiprocessing.cpu_count()):\", multiprocessing.cpu_count())\n",
    "\n",
    "def check_gpu_pytorch():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA está disponible. Número de GPUs:\", torch.cuda.device_count())\n",
    "        print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"CUDA no está disponible en PyTorch.\")\n",
    "\n",
    "def check_gpu_tensorflow():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(\"GPUs disponibles en TensorFlow:\", len(gpus))\n",
    "        for gpu in gpus:\n",
    "            print(\"Nombre de la GPU:\", gpu.name)\n",
    "    else:\n",
    "        print(\"CUDA no está disponible en TensorFlow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cpus()\n",
    "    check_gpu_pytorch()\n",
    "    check_gpu_tensorflow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar y Preprocesar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto Original</th>\n",
       "      <th>Texto Corregido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esto es importante para cada adulto o cada per...</td>\n",
       "      <td>esto es importante para cada adulto o cada per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no salió todo como queríamos fue un buen año m...</td>\n",
       "      <td>no salió todo como queríamos, fue un buen año....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a lo largo de este top verás retratadas siete ...</td>\n",
       "      <td>a lo largo de este top verás retratadas siete ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>te prevengo querida audiencia los próximos min...</td>\n",
       "      <td>te prevengo, querida audiencia, los próximos m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>número siete Benjamin Solari parravicini fue u...</td>\n",
       "      <td>número siete: Benjamín Solari Parravicini fue ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Texto Original  \\\n",
       "0  esto es importante para cada adulto o cada per...   \n",
       "1  no salió todo como queríamos fue un buen año m...   \n",
       "2  a lo largo de este top verás retratadas siete ...   \n",
       "3  te prevengo querida audiencia los próximos min...   \n",
       "4  número siete Benjamin Solari parravicini fue u...   \n",
       "\n",
       "                                     Texto Corregido  \n",
       "0  esto es importante para cada adulto o cada per...  \n",
       "1  no salió todo como queríamos, fue un buen año....  \n",
       "2  a lo largo de este top verás retratadas siete ...  \n",
       "3  te prevengo, querida audiencia, los próximos m...  \n",
       "4  número siete: Benjamín Solari Parravicini fue ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo Excel\n",
    "file_path = '/home/jovyan/work/transcripciones_frases_v5.xlsx'\n",
    "\n",
    "# Cargamos el archivo Excel\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Verificar que las columnas existen\n",
    "assert 'Texto Original' in df.columns, \"La columna 'Texto Original' no existe en el archivo Excel\"\n",
    "assert 'Texto Corregido' in df.columns, \"La columna 'Texto Corregido' no existe en el archivo Excel\"\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir Datos en Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto Original</th>\n",
       "      <th>Texto Corregido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Y hoy en día y más aún con el futuro que se vi...</td>\n",
       "      <td>y hoy en día, y más aún con el futuro que se v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Muchos de estos omicidios se dan cuando ven qu...</td>\n",
       "      <td>muchos de estos homicidios se dan cuando ven q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>este Wow ha sido de mis favoritos sobre todo p...</td>\n",
       "      <td>este, wow, ha sido de mis favoritos, sobre tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>Bien pues una de estas ramas importantes en el...</td>\n",
       "      <td>bien, pues una de estas ramas importantes en e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>¡Wow, ordenador, qué buscaba! Estaba intentand...</td>\n",
       "      <td>¡Wow, ordenador, qué buscaba! Estaba intentand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Texto Original  \\\n",
       "1769  Y hoy en día y más aún con el futuro que se vi...   \n",
       "1915  Muchos de estos omicidios se dan cuando ven qu...   \n",
       "631   este Wow ha sido de mis favoritos sobre todo p...   \n",
       "1814  Bien pues una de estas ramas importantes en el...   \n",
       "1670  ¡Wow, ordenador, qué buscaba! Estaba intentand...   \n",
       "\n",
       "                                        Texto Corregido  \n",
       "1769  y hoy en día, y más aún con el futuro que se v...  \n",
       "1915  muchos de estos homicidios se dan cuando ven q...  \n",
       "631   este, wow, ha sido de mis favoritos, sobre tod...  \n",
       "1814  bien, pues una de estas ramas importantes en e...  \n",
       "1670  ¡Wow, ordenador, qué buscaba! Estaba intentand...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el Modelo y el Tokenizador\n",
    "\n",
    "Usaremos el modelos T5. Tokenizamos antes de introducir el texto en el modelo con T5Tokenizer \n",
    "Se puede usar 't5-base' o 't5-large' según lo preciso (más tiempo de entrenamiento) que queramos entrenar el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "# model_name = 't5-base'  # Puedes usar 't5-small', 't5-large', etc.\n",
    "model_name = (\"vgaraujov/t5-base-spanish\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2801 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2801/2801 [00:00<00:00, 2813.61 examples/s]\n",
      "Map: 100%|██████████| 701/701 [00:00<00:00, 3533.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Definir prefijo y longitudes máximas\n",
    "prefix = \"corregir: \"\n",
    "max_input_length = 300\n",
    "max_target_length = 300\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples['Texto Original']]\n",
    "    targets = [ex for ex in examples['Texto Corregido']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Aplicar preprocesamiento a los datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del dataset de entrenamiento después del preprocesamiento:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2801\n",
      "})\n",
      "Ejemplo de datos preprocesados:\n",
      "{'input_ids': [5695, 33, 94, 253, 10, 121, 9, 27, 350, 16, 11, 550, 7, 15, 750, 19, 55, 322, 179, 3, 749, 1106, 9, 147, 8, 12, 60, 3, 13, 135, 8, 57, 25, 31569, 22, 23, 1944, 35, 10, 51, 9081, 31, 16352, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [9, 253, 10, 121, 4, 9, 27, 350, 16, 11, 550, 7, 15, 750, 4, 19, 55, 322, 179, 3, 749, 1106, 38, 153, 147, 8, 12, 60, 3, 13, 135, 8, 4, 57, 25, 31569, 53, 22, 23, 1944, 35, 10, 51, 9081, 4, 31, 16352, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Estructura del dataset de validación después del preprocesamiento:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 701\n",
      "})\n",
      "Ejemplo de datos preprocesados:\n",
      "{'input_ids': [5695, 33, 21, 10088, 3, 5160, 21, 582, 67, 1775, 158, 1826, 7122, 146, 2348, 10721, 67, 1775, 158, 146, 295, 2158, 2421, 263, 13, 12, 973, 15349, 8, 9, 554, 3750, 5406, 146, 362, 58, 153, 146, 295, 2158, 2421, 263, 93, 13, 1301, 3, 2602, 698, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [21, 10088, 3, 5160, 4, 21, 582, 5, 381, 1775, 158, 4, 205, 1272, 7122, 119, 2726, 2348, 4, 54, 8459, 59, 381, 1775, 158, 4, 205, 1211, 295, 2158, 119, 2421, 263, 13, 7125, 24493, 8, 9, 554, 3750, 5406, 5, 54, 485, 362, 59, 205, 277, 153, 4, 146, 295, 2158, 119, 2421, 263, 93, 13, 1301, 3, 2602, 698, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Imprimir la estructura del dataset después del preprocesamiento\n",
    "print(\"Estructura del dataset de entrenamiento después del preprocesamiento:\")\n",
    "print(train_dataset)\n",
    "print(\"Ejemplo de datos preprocesados:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "print(\"Estructura del dataset de validación después del preprocesamiento:\")\n",
    "print(val_dataset)\n",
    "print(\"Ejemplo de datos preprocesados:\")\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Crear el DataCollator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Definir la métrica\n",
    "metric = evaluate.load('sacrebleu')\n",
    "\n",
    "# Definir la función postprocess_text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# Definir la función compute_metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Definir una clase personalizada para el Trainer\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\", max_new_tokens=300):\n",
    "        self.model.config.max_length = max_new_tokens\n",
    "        return super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "\n",
    "# Añadir logs para verificar los datos antes del entrenamiento\n",
    "def check_data(dataset):\n",
    "    for example in dataset:\n",
    "        input_ids = example.get(\"input_ids\", None)\n",
    "        labels = example.get(\"labels\", None)\n",
    "        if input_ids is None or labels is None:\n",
    "            print(\"Error: 'input_ids' o 'labels' no encontrados en el ejemplo\", example)\n",
    "            continue\n",
    "        if any(id >= tokenizer.vocab_size or id < 0 for id in input_ids):\n",
    "            print(\"Error: input_ids fuera de rango\", input_ids)\n",
    "        if any(id >= tokenizer.vocab_size or (id < 0 and id != -100) for id in labels):\n",
    "            print(\"Error: labels fuera de rango\", labels)\n",
    "\n",
    "# Verificar datos de entrenamiento y evaluación\n",
    "check_data(train_dataset)\n",
    "check_data(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurar y Entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='397' max='22408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  397/22408 01:01 < 57:18, 6.40 it/s, Epoch 0.14/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configurar y Entrenar el Modelo\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/home/jovyan/results',  # Directorio de salida\n",
    "    num_train_epochs=8,                # Épocas\n",
    "    per_device_train_batch_size=1,      # Tamaño del lote de entrenamiento\n",
    "    per_device_eval_batch_size=1,       # Tamaño del lote de evaluación\n",
    "    warmup_steps=100,                    # Pasos de calentamiento\n",
    "    weight_decay=0.00001,                 # Desintegración del peso\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='/home/jovyan/logs',    # Directorio de logs\n",
    "    eval_strategy=\"epoch\",              # Estrategia de evaluación\n",
    "    save_strategy=\"epoch\",              # Estrategia de guardado\n",
    "    fp16=True,                          # Entrenamiento en punto flotante de 16 bits\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=250,           # Longitud máxima de generación (puedes ajustarlo)\n",
    "    generation_num_beams=15,              # Número de haces de búsqueda (mayor valor menor invencion de palabras),\n",
    ")\n",
    "\n",
    "# Definir el entrenador personalizado\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '/home/jovyan/work/correct_transcription_model_base_v5'\n",
    "tokenizer_save_path = '/home/jovyan/work/correct_transcription_tokenizer_base_v5'\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Cargar la métrica\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return result\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels_batch = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_target_length)\n",
    "        \n",
    "        preds.extend(outputs.cpu().numpy())\n",
    "        labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(preds, labels)\n",
    "    return metrics\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluar el modelo\n",
    "metrics = evaluate_model(model, val_dataloader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, dataloader, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_target_length, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "# Crear DataLoader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Evaluar el modelo\n",
    "val_predictions, val_references = generate_predictions(model, val_dataloader, tokenizer)\n",
    "\n",
    "# SacreBLEU y ROUGE esperan listas de listas para referencias\n",
    "references = [[ref] for ref in val_references]\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = metric.compute(predictions=val_predictions, references=references)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar Predicciones y Evaluar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar predicciones y evaluar el modelo\n",
    "def generate_predictions(model, dataloader, tokenizer, max_length=96, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "val_predictions, val_references = generate_predictions(model, val_dataloader, tokenizer)\n",
    "references = [[ref] for ref in val_references]\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = metric.compute(predictions=val_predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar Predicciones y Evaluar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, tokenizer, dataset, max_length=96, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(len(dataset)):\n",
    "        inputs = tokenizer(dataset[i]['Texto Original'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(pred_text)\n",
    "    return predictions\n",
    "\n",
    "val_predictions = generate_predictions(model, tokenizer, val_dataset)\n",
    "references = val_df['Texto Corregido'].tolist()\n",
    "\n",
    "# SacreBLEU y ROUGE esperan listas de listas para referencias\n",
    "references = [[ref] for ref in references]\n",
    "\n",
    "rouge_score = metric.compute(predictions=val_predictions, references=references)\n",
    "bleu_score = metric.compute(predictions=[pred.split() for pred in val_predictions], references=[[ref.split()] for ref in references])\n",
    "\n",
    "print(\"ROUGE score:\", rouge_score)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
