{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos el script para obtener las transcripciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar transcripcines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripciones extraidas correctamente y guardadas en la carpeta 'transcripciones' (en la carpeta de notebooks).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ejecutamos el script crear_transcripciones.py\n",
    "%run /home/jovyan/work/scripts/crear_transcripciones.py\n",
    "\n",
    "# Para listar archivos en el directorio actual\n",
    "# print(\"Archivos en el directorio actual:\")\n",
    "# print(os.listdir('/home/jovyan/work'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from num2words import num2words\n",
    "\n",
    "def clean_text(text):\n",
    "    # Eliminar etiquetas HTML\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Eliminar los sonidos externos entre corchetes\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Eliminar los sonidos externos marcados con *\n",
    "    text = re.sub(r'\\*.*?\\*', '', text)\n",
    "    # Eliminar los sonidos externos entre paréntesis\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    # Convertir números a texto en español\n",
    "    text = re.sub(r'\\d+', lambda x: num2words(int(x.group()), lang='es'), text)\n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'[^A-Za-záéíóúüñÁÉÍÓÚÜÑ\\s]', '', text)\n",
    "    # Eliminar múltiples espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "input_directory = '/home/jovyan/work/data/correccion_gramatical'\n",
    "output_directory = '/home/jovyan/work/data/cleaned'\n",
    "\n",
    "# Guardamos los archivos\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            cleaned_text = clean_text(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección ortográfica y gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'El Rubius_Q0xVOwNmx9w.txt' corregido y guardado.\n",
      "Archivo 'Yuya_wd_MQ3os0M0.txt' corregido y guardado.\n",
      "Archivo 'Luisito Comunica_2at1uc0LfQY.txt' corregido y guardado.\n",
      "Archivo 'Luisito Comunica_w-pIBtP21vc.txt' corregido y guardado.\n",
      "Archivo 'El Rubius_bXSIpq11MfE.txt' corregido y guardado.\n",
      "Archivo 'Yuya_ldATTcJpIpE.txt' corregido y guardado.\n",
      "Archivo 'DrossRotzank_6aswzDXxj7U.txt' corregido y guardado.\n",
      "Archivo 'DrossRotzank_q7Lo6SLfapg.txt' corregido y guardado.\n",
      "Corrección gramatical completada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "GPT = False\n",
    "\n",
    "# Directorios de entrada y salida\n",
    "input_directory = '/home/jovyan/work/data/transcripciones'\n",
    "output_directory = '/home/jovyan/work/data/correccion_gramatical'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Limitamos el texto a 500 caracteres\n",
    "def limit_text(text, max_length=250):\n",
    "    return text[:max_length]\n",
    "\n",
    "# Función para corregir texto usando GPT\n",
    "if GPT:\n",
    "    model_name = 'gpt2-large'\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def correct_text_gpt(text):\n",
    "        inputs = tokenizer.encode(\"Correct this sentence: \" + text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_length=500, num_return_sequences=1)\n",
    "        corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return corrected_text\n",
    "\n",
    "# Función para corregir texto usando la API de LanguageTool\n",
    "else:\n",
    "    def correct_text_api(text, language='es', retries=3, backoff=2):\n",
    "        url = 'https://api.languagetool.org/v2/check'\n",
    "        data = {\n",
    "            'text': text,\n",
    "            'language': language\n",
    "        }\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.post(url, data=data)\n",
    "                response.raise_for_status()  # Lanza una excepción para códigos de estado HTTP 4xx/5xx\n",
    "                return response.json()\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error en la solicitud a LanguageTool API: {e}. Intento {attempt + 1} de {retries}.\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(backoff ** attempt)  # Exponencial backoff\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    # Función para aplicar correcciones preservando la estructura del texto\n",
    "    def apply_corrections(text, matches):\n",
    "        corrections = []\n",
    "        for match in matches['matches']:\n",
    "            if match['replacements']:\n",
    "                correction = {\n",
    "                    'offset': match['offset'],\n",
    "                    'length': match['length'],\n",
    "                    'replacement': match['replacements'][0]['value']\n",
    "                }\n",
    "                corrections.append(correction)\n",
    "        \n",
    "        corrections.sort(key=lambda x: x['offset'], reverse=True)\n",
    "        \n",
    "        for correction in corrections:\n",
    "            start = correction['offset']\n",
    "            end = start + correction['length']\n",
    "            text = text[:start] + correction['replacement'] + text[end:]\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Procesar los archivos en el directorio de entrada\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Limitar el texto a 500 caracteres\n",
    "            text = limit_text(text)\n",
    "            try:\n",
    "                if GPT:\n",
    "                    corrected_text = correct_text_gpt(text)\n",
    "                else:\n",
    "                    matches = correct_text_api(text)\n",
    "                    corrected_text = apply_corrections(text, matches)\n",
    "\n",
    "                # Guardar el texto corregido en el directorio de salida\n",
    "                with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                    out_file.write(corrected_text)\n",
    "                print(f\"Archivo '{filename}' corregido y guardado.\")\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo corregir el archivo '{filename}': {e}\")\n",
    "\n",
    "print(\"Corrección gramatical completada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# input_directory = '/home/jovyan/work/data/corrected'\n",
    "input_directory = '/home/jovyan/work/data/normalized'\n",
    "output_directory = '/home/jovyan/work/data/tokenized'\n",
    "\n",
    "# Guardamos los archivos tokenizados\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize(text)\n",
    "            with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Resultados\n",
    "### Comparación de Textos Originales y Corregidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ballo vale eh No hay tiempo para intros hay que buscar a a Don Ramón que está perdido en el mar eh Espero que siga vivo por favor que esté vivo no sé por qué me he jugado la vida por un barco la verdad cuando es casi gratis de hacer pero vale dónde m\n",
      "Corrected: ballo vale eh no hay tiempo para antros hay que buscar a don ramón que está perdido en el mar eh espero que siga vivo por favor que esté vivo no sé por qué me he jugado la vida por un barco la verdad cuando es casi gratis de hacer pero vale dónde me \n",
      "==================================================\n",
      "Original:  ahora podrás como es tan diario yo soy yo ya juegan amigos a díaz ha llegado el momento de volvernos a encontrar ahora sí que no procrastinar realmente no podía usar no me acomodaba pasaron muchas cosas ha sido hermoso y caótico también esto que ven\n",
      "Corrected: uegan amigos a díaz ha llegado el momento de volvernos a encontrar ahora sí que no procrastinar realmente no podía usar no me acomodaba pasaron muchas cosas ha sido hermoso y caótico también esto que ven acá es que marc no le gustan los chupones ento\n",
      "==================================================\n",
      "Original: na suite digna de un hotel\n",
      "de cinco estrellas. No por nada cuesta más de $6000 la noche. Hoy vamos a vivir la experiencia de uno\n",
      "de los trenes más Lujosos del Mundo y el que presume ser el más lujoso\n",
      "de todo el continente americano. Este es el explor\n",
      "Corrected: ite digna de un hotel de cinco estrellas no por nada cuesta más de seis mil la noche hoy vamos a vivir la experiencia de uno de los trenes más lujosos del mundo y el que presume ser el más lujoso de todo el continente americano este es el explorador \n",
      "==================================================\n",
      "Original: es tú Ay Sí [Música] amigos amigas Bienvenidos a una edición más de Luisillo el Gordillo en esta ocasión nos hemos transportado al país que digo país al subcontinente de India un país masivo caótico con muchísima cultura muchísima tradición en donde \n",
      "Corrected: es tú ay sí amigos amigo bienvenido a una edición más de luisillo el gordillo en esta ocasión nos hemos transportado al país que digo país al subcontinente de india un país masivo caótico con muchísima cultura muchísima tradición en donde por supuest\n",
      "==================================================\n",
      "Original:  otro día Como sabéis algunos me mandaron una caja rollo puzle eh Y Misteriosa que tenía un tesoro dentro y tuve que abrirla usando mi Ingenio hemos resuelto el puzle chicos y bueno Pues resulta ser que dentro de la caja ponía que eh tenía que venir \n",
      "Corrected:  otro día como sabéis algunos me mandaron una caja rollo puzle eh y misteriosa que tenía un tesoro dentro y tuve que abrirla usando mi ingenio hemos resuelto el puzle chicos y bueno pues resulta ser que dentro de la caja ponía que eh tenía que venir \n",
      "==================================================\n",
      "Original: os para usarnos la juntos un ratito para puras con esta en el día de hoy yo soy chucha pero que estén muy bien oigan ahora sí que siento que hace un buen rato nos saludamos los voy a actualizar es cierto se está controlando bien no más esto miren est\n",
      "Corrected: sarnos el junto un ratito para puras con esta en el día de hoy yo soy chucha pero que estén muy bien oigan ahora sí que siento que hace un buen rato nos saludamos los voy a actualizar es cierto se está controlando bien no más esto miren esto por acá \n",
      "==================================================\n",
      "Original: marca el fin de un ciclo en el que podemos mirar atrás y ponderar no salió todo como queríamos fue un buen año mucho más importante aún la pregunta que nos encoge el corazón porque nos da Esperanza pero también miedo cómo será el año que comienza a l\n",
      "Corrected: fin de un ciclo en el que podemos mirar atrás y ponderar no salió todo como queríamos fue un buen año mucho más importante aún la pregunta que nos encoge el corazón porque nos da esperanza pero también miedo cómo será el año que comienza a lo largo d\n",
      "==================================================\n",
      "Original: inación a quienes se les retorcían las tripas poniéndose en el lugar de las personas que sufrieron aquellas hórrido me causó placer por eso he aquí la secuela una aún mucho más grotesca que el primer top lo que verás a continuación todo peor que las \n",
      "Corrected: inación a quienes se les retorcían las tripas poniéndose en el lugar de las personas que sufrieron aquel hórrido me causó placer por eso he aquí la secuela una aún mucho más grotesca que el primer top lo que verás a continuación todo peor que las más\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Función para leer los caracteres de un archivo desde un índice n hasta m\n",
    "def read_chars_from_n_to_m(file_path, n, m):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        return text[n:m]\n",
    "\n",
    "# Función para mostrar la comparación entre el texto original y el corregido\n",
    "def compare_texts(original, corrected):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Función principal para comparar textos en archivos\n",
    "def compare_files(original_directory, corrected_directory, start_char, end_char):\n",
    "    # Obtener los nombres de los archivos en el directorio original\n",
    "    file_names = [f for f in os.listdir(original_directory) if f.endswith('.txt')]\n",
    "\n",
    "    # Aplicar la comparación a los archivos de texto\n",
    "    for file_name in file_names:\n",
    "        original_path = os.path.join(original_directory, file_name)\n",
    "        corrected_path = os.path.join(corrected_directory, file_name)\n",
    "        \n",
    "        # Verificar que el archivo corregido exista\n",
    "        if os.path.exists(corrected_path):\n",
    "            original_text = read_chars_from_n_to_m(original_path, start_char, end_char)\n",
    "            corrected_text = read_chars_from_n_to_m(corrected_path, start_char, end_char)\n",
    "            \n",
    "            # Comparar los textos\n",
    "            compare_texts(original_text, corrected_text)\n",
    "        else:\n",
    "            print(f\"Archivo corregido no encontrado para: {file_name}\")\n",
    "\n",
    "# Directorios de transcripciones originales y corregidas\n",
    "original_directory = '/home/jovyan/work/data/transcripciones'\n",
    "corrected_directory = '/home/jovyan/work/data/tokenized'\n",
    "\n",
    "# Índices de caracteres a comparar\n",
    "start_char = 100\n",
    "end_char = 350\n",
    "\n",
    "# Llamada a la función principal\n",
    "compare_files(original_directory, corrected_directory, start_char, end_char)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
